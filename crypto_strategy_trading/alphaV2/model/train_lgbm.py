"""
[Alpha V2 - Model æ¨¡å—] LightGBM å†³ç­–æ¨¡å‹è®­ç»ƒ
-------------------------------------------
åŠŸèƒ½è¯´æ˜ï¼š
    è¿™ä¸ªæ–‡ä»¶å±äº "Model (å¤§è„‘)" ç¯èŠ‚ã€‚
    å®ƒçš„ä½œç”¨æ˜¯è®­ç»ƒä¸€ä¸ªäººå·¥æ™ºèƒ½æ¨¡å‹ (LightGBM)ï¼Œè®©å®ƒå­¦ä¹ å¦‚ä½•æ ¹æ®
    "Mining" ç¯èŠ‚ç”Ÿäº§çš„æŒ‡æ ‡æ¥é¢„æµ‹æœªæ¥çš„ä»·æ ¼æ¶¨è·Œã€‚

    åŒæ—¶ï¼Œå®ƒä¹Ÿæ˜¯ä¸€ä¸ª "å› å­æŒ–æ˜æœº"ã€‚é€šè¿‡æŸ¥çœ‹æ¨¡å‹è®¤ä¸ºå“ªäº›ç‰¹å¾æœ€é‡è¦
    (Feature Importance)ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°å“ªäº›æŒ‡æ ‡æ˜¯çœŸæ­£æœ‰æ•ˆçš„ Alpha å› å­ã€‚

å·¥ä½œæµç¨‹ï¼š
    1. åŠ è½½æ•°æ® (Load Data)
    2. ç”Ÿæˆç‰¹å¾ (Generate Features - è°ƒç”¨ FeatureFactory)
    3. æ ‡è®°ç›®æ ‡ (Labeling): æ¯”å¦‚ "æœªæ¥1å°æ—¶æ¶¨å¹… > 1%" è®°ä¸º 1ï¼Œå¦åˆ™ä¸º 0ã€‚
    4. è®­ç»ƒæ¨¡å‹ (Training): è®© AI å­¦ä¹ ç‰¹å¾ä¸ç›®æ ‡ä¹‹é—´çš„å…³ç³»ã€‚
    5. è¾“å‡ºç»“æœ: ä¿å­˜æ¨¡å‹æ–‡ä»¶ï¼Œå¹¶æ‰“å°å‡ºæœ€é‡è¦çš„ Top 10 å› å­ã€‚
"""
import os
import sys
import pandas as pd
import numpy as np
import logging
import json

# å°è¯•å¯¼å…¥ lightgbm
try:
    import lightgbm as lgb
except ImportError:
    print("âŒ é”™è¯¯: ç¼ºå°‘ 'lightgbm' åº“ã€‚")
    print("è¯·è¿è¡Œ: pip install lightgbm")
    sys.exit(1)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from alphaV2.mining.feature_factory import FeatureFactory
from alphaV2.optimization.optuna_martingale import load_data # å¤ç”¨æ•°æ®åŠ è½½å‡½æ•°

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def train_model(symbol, data_dir):
    print(f"\nğŸ§  å¼€å§‹è®­ç»ƒå†³ç­–æ¨¡å‹ (Symbol: {symbol})...")
    
    # 1. åŠ è½½æ•°æ®
    df = load_data(symbol, data_dir)
    if df is None:
        print("æœªæ‰¾åˆ°æ•°æ®ã€‚")
        return

    # 2. ç”Ÿæˆç‰¹å¾ (X)
    print("æ­£åœ¨ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾...")
    ff = FeatureFactory()
    df_features = ff.generate_features(df)
    
    # 3. åˆ›å»ºé¢„æµ‹ç›®æ ‡ (y)
    # ç›®æ ‡: é¢„æµ‹æœªæ¥ 12 ä¸ªå‘¨æœŸ (ä¾‹å¦‚ 5åˆ†é’ŸKçº¿ * 12 = 1å°æ—¶) çš„æ”¶ç›Šç‡
    prediction_horizon = 12 
    future_ret = df['close'].shift(-prediction_horizon) / df['close'] - 1
    
    # åˆ†ç±»ç›®æ ‡: å¦‚æœæœªæ¥æ¶¨å¹… > 1%ï¼Œæ ‡è®°ä¸º 1 (ä¹°å…¥æœºä¼š)ï¼Œå¦åˆ™ä¸º 0
    # è¿™æ˜¯ä¸€ä¸ª "ç‹™å‡»æ‰‹" é€»è¾‘: åªæœ‰é«˜æ¦‚ç‡å¤§æ¶¨æ—¶æ‰å‡ºæ‰‹
    threshold = 0.01
    df_features['target'] = (future_ret > threshold).astype(int)
    
    # åˆ é™¤åŒ…å« NaN çš„è¡Œ (ä¸»è¦æ˜¯æœ€åå‡ è¡Œæ²¡æœ‰æœªæ¥æ•°æ®)
    df_features = df_features.dropna()
    
    # åˆ†ç¦»ç‰¹å¾ (X) å’Œ ç›®æ ‡ (y)
    # æ’é™¤éç‰¹å¾åˆ—
    X = df_features.drop(columns=['target', 'open', 'high', 'low', 'close', 'volume', 'timestamp'], errors='ignore')
    y = df_features['target']
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† (æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†ï¼Œå‰ 80% è®­ç»ƒï¼Œå 20% æµ‹è¯•)
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}, æµ‹è¯•é›†å¤§å°: {len(X_test)}")
    print(f"è®­ç»ƒé›†ä¸­æ­£æ ·æœ¬æ¯”ä¾‹ (ä¹°å…¥æœºä¼š): {y_train.mean():.2%}")
    
    # 4. è®­ç»ƒ LightGBM æ¨¡å‹
    print("å¼€å§‹è®­ç»ƒ LightGBM...")
    train_data = lgb.Dataset(X_train, label=y_train)
    test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
    
    params = {
        'objective': 'binary',       # äºŒåˆ†ç±»é—®é¢˜
        'metric': 'auc',             # è¯„ä¼°æŒ‡æ ‡: AUC
        'boosting_type': 'gbdt',     # æ¢¯åº¦æå‡æ ‘
        'num_leaves': 31,            # æ ‘çš„å¤æ‚åº¦
        'learning_rate': 0.05,       # å­¦ä¹ ç‡
        'feature_fraction': 0.9      # æ¯æ¬¡åˆ†è£‚åªéšæœºé€‰ 90% çš„ç‰¹å¾ (é˜²æ­¢è¿‡æ‹Ÿåˆ)
    }
    
    bst = lgb.train(
        params,
        train_data,
        num_boost_round=100,
        valid_sets=[test_data],
        callbacks=[lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation(10)]
    )
    
    # 5. ç‰¹å¾é‡è¦æ€§ (æŒ–æ˜ç»“æœ)
    print("\nğŸ’ Top 10 æœ€æœ‰æ•ˆçš„ Alpha å› å­ (ç‰¹å¾é‡è¦æ€§):")
    importance = bst.feature_importance(importance_type='gain')
    feature_names = X.columns.tolist()
    
    # åˆ›å»º DataFrame å±•ç¤ºç»“æœ
    imp_df = pd.DataFrame({'feature': feature_names, 'importance': importance})
    imp_df = imp_df.sort_values('importance', ascending=False)
    
    print(imp_df.head(10))
    
    # ä¿å­˜æ¨¡å‹
    model_path = os.path.join(os.path.dirname(__file__), f'lgbm_model_{symbol}.txt')
    bst.save_model(model_path)
    print(f"\næ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")
    
    return imp_df

if __name__ == "__main__":
    base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    data_dir = os.path.join(base_dir, 'backtest', 'data')
    
    # é»˜è®¤æµ‹è¯• PEPE
    train_model('1000PEPEUSDT', data_dir)
